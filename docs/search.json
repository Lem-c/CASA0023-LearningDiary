[
  {
    "objectID": "index.html#casa0023-remotely-sensing-cities-and-environments",
    "href": "index.html#casa0023-remotely-sensing-cities-and-environments",
    "title": "Remotely sensing learning diary",
    "section": "CASA0023 Remotely Sensing Cities and Environments",
    "text": "CASA0023 Remotely Sensing Cities and Environments\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "week_1.html#summary",
    "href": "week_1.html#summary",
    "title": "Week1 - Intro to Remote Sensing",
    "section": "Summary:",
    "text": "Summary:\nFirst week, OK, let‚Äôs go.\n\nRemote Sensing\nHigh-resolution remote sensing images, paired with advanced analysis techniques, enable precise and timely monitoring of urban environments.\nPassive sensors detect natural energy, typically sunlight, while active sensors emit their own signals, such as electromagnetic waves, and measure their return. Examples include photographic and infrared sensors as passive; radar and sonar as active.\nThe nature wavelegth can be calculated through:\n\\[\n\\Lambda(\\text{wavelength}) = \\frac{c (\\text{velocity of light})}{v (\\text{frequency})}\n\\]\n\n\nScatter\nBefore reaching the sensor, energy can be absorbed by surfaces or scattered by atmospheric particles. The sky‚Äôs blue color results from blue light‚Äôs shorter wavelengths scattering more easily. As the sun‚Äôs angle changes, increased distance reduces blue light scattering to our eyes, allowing longer wavelengths like reds and oranges to dominate. This visibility is due to atmospheric molecules scattering light, with other colors being scattered away, leaving primarily orange or red hues visible.\nEnergy on the way to the sensor may be absorbed or scattered by atmospheric particles, explaining the sky‚Äôs blueness due to the easier scattering of blue light‚Äôs shorter wavelengths. With the sun‚Äôs angle shift, the greater distance prevents blue light scattering, making red and orange hues, which have the longest wavelengths, more visible. This phenomenon occurs as the atmosphere scatters light, allowing only orange or red to reach our eyes.\nEnergy heading towards the sensor is either absorbed or atmospherically scattered. The sky appears blue because blue light‚Äôs short wavelengths scatter more efficiently. However, as the sun shifts, the increased scattering distance reduces blue visibility, making longer wavelengths like red and orange more apparent. This effect is facilitated by atmospheric scattering, leaving predominantly orange or red light visible.\n\n\nResolutions\n\nSpatial: Raster cell size ranges from 10cm to several kilometers.\nSpectral: This aspect involves detecting various wavelengths across the electromagnetic spectrum, extending beyond visible light. The color of objects is determined by the wavelengths they reflect, with the rest being absorbed or scattered. Observation limitations arise from wavelengths absorbed by water vapor, ozone, and other gases. Spectral resolution is categorized by the number of detectable bands. Spectral reflectance measurement is not exclusive to remote sensing but can also be performed with spectroradiometers in laboratories or on the field, necessitating calibration against a pure white reference panel.\nRadiometric: Revisit times of sensors vary, with lower resolutions indicating larger pixel sizes. Remote sensing utilizes fluorescence to identify materials and evaluate conditions by analyzing wavelength emissions after radiation exposure.\nTemporal: Different sensors vary in energy sensitivity, with higher resolutions providing greater detail (e.g., 8 bit = 256 values, 4 bit = 16 values)."
  },
  {
    "objectID": "week_1.html#application",
    "href": "week_1.html#application",
    "title": "Week1 - Intro to Remote Sensing",
    "section": "Application",
    "text": "Application\nW√≥jtowicz et al.(W√≥jtowicz et al. 2016) apply remote sensing technologies within agriculture, and Szpakowski and Jensen (Szpakowski and Jensen 2019) investigate its implications for fire ecology. The former paper discusses remote sensing‚Äôs critical role in agricultural applications such as crop yield prediction, nutrient level assessment, and weed management. It highlights the utility of vegetation indices, with a focus on NDVI, for evaluating crop health and progress. Furthermore, the paper reviews remote sensing‚Äôs diverse platforms, including terrestrial, aerial, and satellite, each offering unique benefits in terms of spatial and spectral resolution.\nRemote sensing in agriculture is not without its challenges:\nEnvironmental Variability: The efficacy of remote sensing data can be undermined by environmental changes, including lighting and weather, posing difficulties for consistent data measurement. Signal versus Noise Distinction: The task of differentiating crop stress signals from the ‚Äúnoise‚Äù created by other elements, such as soil and non-photosynthetic plant material, is notably challenging. Technological Demands: The requirement for high-resolution data to monitor small-scale or early agricultural developments necessitates the use of sophisticated technology, which can be prohibitively expensive.\n\n\n\n(Szpakowski and Jensen 2019)\n\n\nThe review by Szpakowski and Jensen (Szpakowski and Jensen 2019) offers an extensive evaluation of remote sensing applications in fire ecology, discussing aspects such as fire risk assessment, fuel mapping, active fire detection, estimation of burned areas, assessment of burn severity, and the characterization and monitoring of post-fire vegetation recovery. It focuses on the use of spectral sensors, lidar, and the innovative application of UAS technologies, providing a discussion of current methodologies and showcasing relevant research examples.\nAddressing the challenges of environmental variability, signal interpretation, and technological advancement is crucial for unlocking its full potential. Further research and development in this field are essential for enhancing its application in precision agriculture and beyond.\n\nTry SNAP\nJust follow the steps in practical one by one, let‚Äôs do it.\nLoad the London ward shapefile for masking. When I first came to SNAP, it is totally a strange thing for me. After loading the zip file and converted to default RGB filed, the area I chose looks like this ?fig-snap1.\n \nAhhh, a huge area of white. Actually,it confused me a lot and I can not get any useful information from it, and I spend an hour to download all the files (sad face). Till I figure out these are clouds and I choose a band that shows the satellite view, I finally tells the differences between soil and green covered area and water (sea). Even though I can‚Äôt use these data, I believe they will be useful in the future."
  },
  {
    "objectID": "week_1.html#reflection",
    "href": "week_1.html#reflection",
    "title": "Week1 - Intro to Remote Sensing",
    "section": "Reflection",
    "text": "Reflection\nWeek one is quite interesting for me, learning sensor and wave phythsics related knowledge. Reflecting on this week‚Äôs content, I‚Äôm struck by the elegance of remote sensing and its practical applications. It‚Äôs fascinating how we can use wavelengths and frequencies to unveil details of the Earth‚Äôs surface, like a cosmic detective uncovering secrets from space. The challenges, like differentiating vital data from atmospheric noise and the cost of technology, remind me that this field is as grounded in real-world issues as it is in theoretical physics.\nWhat‚Äôs most compelling is how this knowledge isn‚Äôt just for academic musings. It‚Äôs key to tangible solutions, like improving agricultural yields or assessing wildfire risks. While my first foray with tools like SNAP was baffling, it was a necessary step towards proficiency. This skill set, although occasionally esoteric, has clear potential for future applications, possibly revolutionizing fields like precision farming and disaster management.\nI‚Äôm realizing that even when specific technologies may not be immediately useful, the underlying principles are universally valuable. This week‚Äôs lessons have reinforced my belief that remote sensing is an essential, transformative element of modern science. üî•"
  },
  {
    "objectID": "week_1.html#references",
    "href": "week_1.html#references",
    "title": "Week1 - Intro to Remote Sensing",
    "section": "References",
    "text": "References\n\n\n\n\nSzpakowski, David M, and Jennifer LR Jensen. 2019. ‚ÄúA Review of the Applications of Remote Sensing in Fire Ecology.‚Äù Remote Sensing 11 (22): 2638.\n\n\nW√≥jtowicz, Marek, Andrzej W√≥jtowicz, Jan Piekarczyk, et al. 2016. ‚ÄúApplication of Remote Sensing Methods in Agriculture.‚Äù Communications in Biometry and Crop Science 11 (1): 31‚Äì50."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "Week2 - Try the presentation",
    "section": "",
    "text": "Xaringan Presentation"
  },
  {
    "objectID": "week_3.html#summary",
    "href": "week_3.html#summary",
    "title": "Week3 - Corrections (Data Fusion: Principles and Methods)",
    "section": "Summary",
    "text": "Summary\nData Fusion, originally used in military domains, is now a critical part of various fields including multi-source image composition, robotics, unmanned aerial vehicles, image analysis, and more. It involves the integration of multi-source remote sensing image data, enhancing the accuracy, completeness, and reliability of data interpretation.\n\nPrinciples and Process of Data Fusion\nData fusion in remote sensing images typically follows a two-step process:\n\n1. Preprocessing\nInvolves geometric correction, atmospheric correction, radiometric correction, and spatial registration of remote sensing images.\n\n\n2. Data Fusion\nSelecting suitable fusion algorithms based on the purpose and level of fusion to synthesize spatially registered data or extracted features, leading to more accurate target representation or estimation.\n\n\n\nClassification and Methods of Data Fusion\nData fusion methods for remote sensing images are categorized into:\n\nPixel-level Fusion: A low-level fusion maintaining high accuracy but with limitations in efficiency and analysis capability.\nFeature-level Fusion: A medium-level fusion focusing on extracting and integrating features from different sources.\nDecision-level Fusion: The highest level of fusion providing strong fault tolerance and requiring high processing capabilities.\n\nVarious data fusion methods include algebraic methods, image regression, principal component transformation (PCT), K-T transformation, wavelet transformation, and IHS transformation."
  },
  {
    "objectID": "week_3.html#week-practical",
    "href": "week_3.html#week-practical",
    "title": "Week3 - Corrections (Data Fusion: Principles and Methods)",
    "section": "Week practical",
    "text": "Week practical\nFinally some R related things, so, I‚Äôll do it step by step. Firstly, the two areas chose as follow (It‚Äôs just two randomly areas chose around South Asia, looks like most of area are yellow sand):\nThe process begins with choosing specific areas for analysis, which shows contrasts in the landscape, such as water bodies.\n\nThen merge the Satellite data to combine different spectral bands or time points to create a comprehensive view of the chosen areas. They are quilt confusing, and I‚Äôll figure all bands out in later chapters.\n\nCalculation Normalized Difference Vegetation Index (NDVI). The NDVI values range from -0.40 to 0.40, with green areas indicating healthier vegetation and brown areas indicating less healthy or sparse vegetation. This image suggests that there are a lot of area might be indicative of barren land, rock, or bare soil with little to no vegetation present. The equation to get NDVI is:\n\\[ NDVI = \\frac{NIR- Red}{NIR + Red}  \\]\n\n\n\nNDVI (Wikipedia contributors 2024) is a computed index that indicates vegetation health. It uses the visible and near-infrared bands of the electromagnetic spectrum to identify live green vegetation.\n\n\nTexture Analysis: This step examines the surface roughness or textural patterns of the landscape, which can be critical for classifying different land cover types like soil and water.\n\nPrincipal Component Analysis (PCA): PCA is a statistical technique used to emphasize variation and bring out strong patterns in a dataset. It transforms the original data into a set of linearly uncorrelated variables known as principal components, with the first principal components accounting for as much of the variability in the data as possible."
  },
  {
    "objectID": "week_3.html#application",
    "href": "week_3.html#application",
    "title": "Week3 - Corrections (Data Fusion: Principles and Methods)",
    "section": "Application",
    "text": "Application\nConsidering data fusion in remote sensing field, a paper I found used Pixel-Level, Decision-Level and Feature-Level fusion techniques (Zhang 2010) to realize it. Pixel-level fusion integrates high-resolution panchromatic data with lower-resolution multispectral data to produce images that combine the best features of both: high spatial resolution and rich spectral information. Feature-level fusion combines features extracted from different sources, such as edges, textures, and other morphological parameters, to enhance classification, object detection, and change detection processes. And decision-level fusion combines the outcomes of multiple classification or detection algorithms to arrive at a final decision, enhancing reliability and accuracy. These techniques enable the extraction of more accurate and detailed information from the Earth‚Äôs surface, facilitating better environmental monitoring and algorithm correctness.\nAs for Atmospheric Corrections, Chavez et al.(Chavez et al. 1996) highlighting its impact for various applications, including vegetation monitoring, urban planning, and climate change studies. It can improve spatial resolution for detecting atmospheric particulates by integrating LiDAR data with optical imagery to enhance the detection of atmospheric particulates and their spatial distribution. This fusion is particularly useful in studying the vertical distribution of aerosols and clouds, providing valuable information for atmospheric correction and climate research. For temporal Data, the fusion of real-time data from multiple sensors, including weather satellites and ground-based observations, supports the development of dynamic atmospheric correction models. These models can adjust for sudden changes in atmospheric conditions, ensuring the accuracy of surface reflectance data for time-sensitive applications. The application of advanced data fusion techniques in atmospheric corrections has led to significant improvements in the quality and reliability of corrected remote sensing imagery."
  },
  {
    "objectID": "week_3.html#reflection",
    "href": "week_3.html#reflection",
    "title": "Week3 - Corrections (Data Fusion: Principles and Methods)",
    "section": "Reflection",
    "text": "Reflection\nFirst, it‚Äôs incredible how I can take images from satellites, correct them for distortions caused by the camera‚Äôs angle and the atmosphere, and then turn them into precise maps of the Earth‚Äôs surface.\nI delved into data fusion, where the goal is to mix data from different sources to create a clearer, more detailed picture. There are different ways to do this, from mixing pixel by pixel, to looking at features like edges and textures, to making big-picture decisions. It‚Äôs like putting together a complex puzzle, with each piece providing more insights into the larger image.\nOne practical application was analyzing parts of South Asia (India). It‚Äôs fascinating to see how areas that mostly looked like yellow sand from afar actually held a lot of details when you zoomed in. Using the Normalized Difference Vegetation Index, or NDVI for short, I could tell where the healthy greenery was and where it wasn‚Äôt, which is super useful for understanding the landscape.\nTexture analysis not just about colors but also how rough or smooth areas are, which can tell you a lot about the terrain. And when I ran a Principal Component Analysis, it was like finding the hidden patterns in the data. It‚Äôs a powerful way to reduce complexity and highlight what‚Äôs really important.\nLooking forward, though, it‚Äôs clear there are challenges ahead. The math behind fusing data from different sources can get tricky and complex. But it‚Äôs also an exciting time, with Geographic Information Systems and smarter processing tools making it possible to monitor our planet in real-time."
  },
  {
    "objectID": "week_3.html#references",
    "href": "week_3.html#references",
    "title": "Week3 - Corrections (Data Fusion: Principles and Methods)",
    "section": "References",
    "text": "References\n\n\n\n\nChavez, Pat S et al. 1996. ‚ÄúImage-Based Atmospheric Corrections-Revisited and Improved.‚Äù Photogrammetric Engineering and Remote Sensing 62 (9): 1025‚Äì35.\n\n\nWikipedia contributors. 2024. ‚ÄúNormalized Difference Vegetation Index.‚Äù 2024. https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index.\n\n\nZhang, Jixian. 2010. ‚ÄúMulti-Source Remote Sensing Data Fusion: Status and Trends.‚Äù International Journal of Image and Data Fusion 1 (1): 5‚Äì24."
  },
  {
    "objectID": "week_4.html",
    "href": "week_4.html",
    "title": "Week4 - Policy Applications",
    "section": "",
    "text": "05 Reading Week: Temperature\nHistory temperature and CO2 emissions in UK"
  },
  {
    "objectID": "week_4.html#summary",
    "href": "week_4.html#summary",
    "title": "Week4 - Policy Applications",
    "section": "Summary",
    "text": "Summary\n\nData from sensors\nSatellites collect a wide array of data using different parts of the electromagnetic spectrum, each revealing unique details about the Earth‚Äôs surface and atmosphere. They capture everything from visible light and near-infrared, which show us images like what our eyes see and help monitor vegetation health and urban growth, to thermal infrared that detects the Earth‚Äôs heat emission for studying temperature changes and identifying wildfires. Microwave sensors, divided into several bands including C, X and L bands (‚ÄúICEYE: Satellite Data,‚Äù n.d.), cut through clouds and vegetation to gather data in any weather, useful for tracking soil moisture and sea temperatures. Radar and Lidar technologies provide detailed images and 3D maps for studying terrain, forest structures, and urban development. There‚Äôs even data from ultraviolet to X-ray bands, though less used for Earth observation, they‚Äôre crucial for studying atmospheric conditions and cosmic phenomena. Together, these diverse data streams allow scientists to explore everything from climate patterns to natural disasters, offering insights essential for understanding and protecting our planet.\n\n\nLand use and Land cover (LULC)\nUrban green spaces (Shahtahmassebi et al. 2021) are analyzed using data from Landsat and Sentinel satellites, alongside LiDAR with high spatial resolution, to study vegetation health and accessibility to green areas, as well as to evaluate urban planning efforts through tools like Google Street View and Google Earth Engine. For disaster response and preparedness, Sentinel-2 spectral imagery is used to compare pre-event and post-event conditions, conduct view-shed analysis, and utilize clustering techniques to assist in building 3D models for events like the Beirut explosion, aiding in efficient and informed disaster management strategies.\n\n\nSynthetic Aperture Radar (SAR)\nIts capabilities as an active sensing system that captures surface texture data and can see through weather and clouds due to different wavelengths and polarizations. SAR operates in multiple bands like X, C, S, L, and P (‚ÄúWhat Is SAR?‚Äù n.d.), each with varying applications from high-resolution urban monitoring to biomass and vegetation mapping. The polarization of SAR, whether horizontal, vertical, or dual, affects how it interacts with materials and can aid in differentiating features such as rough surfaces and double-bounce reflections in urban areas. SAR imagery can also undergo various corrections for improved clarity and accuracy. With different wavelengths, SAR can penetrate surfaces to varying degrees, affecting volume scattering and thus enabling diverse applications like terrain mapping and change detection. The slides touch on the use of SAR in urban monitoring, explaining how the unique properties of SAR, such as backscatter and polarization, can be utilized to analyze and understand urban structures and natural environments."
  },
  {
    "objectID": "week_4.html#application",
    "href": "week_4.html#application",
    "title": "Week4 - Policy Applications",
    "section": "Application",
    "text": "Application\n\nCase study: London Plan\nLondon, as one of the world‚Äôs major cities (Greater London Authority 2021), faces a unique set of challenges posed by the urban heat island (UHI) effect‚Äîa phenomenon where urban regions experience higher temperatures than their rural surroundings. Policy SI4 of the London Plan 2021 seeks to mitigate this by guiding new developments to minimize internal overheating and reduce reliance on air conditioning systems.\n\nPolicy SI 4 Managing heat risk (Abercrombie 1944) p354 introduces that Policy SI 4 Managing Heat Risk addresses the adverse impacts of the UHI effect by setting out a cooling hierarchy that includes reducing heat entry, minimizing internal heat generation, and enhancing building heat management through design. This policy is essential in the context of London‚Äôs changing climate, where the city is experiencing higher average temperatures and more severe hot weather events.\nUrban development proposals must mitigate the urban heat island effect through thoughtful design, materials, and green infrastructure. Major developments need an energy strategy to reduce internal overheating and air conditioning reliance, following a cooling hierarchy that prioritizes:\n\n\nReducing heat entry via orientation, shading, and insulation.\nMinimizing internal heat through efficient design.\nManaging internal heat with thermal mass and high ceilings.\nEnabling passive ventilation, followed by mechanical ventilation.\nConsidering active cooling systems only as a last resort.\n\nClimate change has heightened London‚Äôs temperature, exacerbating the urban heat island effect and health risks during extreme heat. New developments must address citywide and building-specific overheating. Strategies include avoiding excessive glazing and promoting passive ventilation. Air conditioning, which contributes to urban heat, should be minimized. The Chartered Institution of Building Services Engineers (CIBSE) provides guidelines for addressing overheating in new and refurbished buildings, ensuring designs are future-proof against climate change.\n\nApplication Criterion: Remotely sensed data, particularly from satellite imagery and thermal mapping, can be instrumental in assessing UHI impacts and guiding urban planning. Such data could inform strategies for building orientation, the placement of green spaces, and the materials used in construction to manage heat risk effectively.\n\n\n\nLandsat pre-processing steps\nIncluded: (i) image re-sampling, (ii) conversion of raw digital values (DN) to top of atmosphere (TOA) reflectance, (iii) cloud/shadow/water screening and quality assessment (QA), and (iv) image normalization‚Äù\nPolicy SI4 offers a progressive framework for sustainable urban development, using remotely sensed data to address London‚Äôs UHI challenge."
  },
  {
    "objectID": "week_4.html#reflection",
    "href": "week_4.html#reflection",
    "title": "Week4 - Policy Applications",
    "section": "Reflection",
    "text": "Reflection\nReflecting on my journey to understand the hot spots in cities has been like unfolding a map that leads from a single point of curiosity to a vast landscape of global environmental challenges. It all started with noticing how some parts of the city felt warmer, which drew a deep dive into learning from various fields. By combining insights from satellite images, exploring city planning policies, and diving into environmental science, I‚Äôve combined together a clearer picture of why cities get so warm and what it means for us.\nThis adventure has taught me the value of looking at problems from different angles. It‚Äôs not just about the heat; it‚Äôs about how we design our cities, the rules that guide our urban development, and the new technologies that let us see our environment in different ways. This mix of personal experience, academic study, and the latest tech has shown me that tackling big challenges requires a mix of knowledge, curiosity, and a willingness to explore.\nIn short, this week has opened my eyes. It‚Äôs shown me that getting to grips with the issue of cities heating up is crucial for building sustainable places for us to live. And through this process, I‚Äôve learned that when we combine our personal observations with approach to learning and technology can make a real impact."
  },
  {
    "objectID": "week_4.html#references",
    "href": "week_4.html#references",
    "title": "Week4 - Policy Applications",
    "section": "References",
    "text": "References\n\n\n\n\nAbercrombie, Patrick. 1944. Greater London Plan. HM Stationery Office.\n\n\nGreater London Authority. 2021. ‚ÄúThe London Plan 2021.‚Äù https://www.london.gov.uk/programmes-strategies/planning/london-plan/new-london-plan/london-plan-2021.\n\n\n‚ÄúICEYE: Satellite Data.‚Äù n.d. https://www.iceye.com/satellite-data.\n\n\nShahtahmassebi, Amir Reza, Chenlu Li, Yifan Fan, Yani Wu, Muye Gan, Ke Wang, Arunima Malik, George Alan Blackburn, et al. 2021. ‚ÄúRemote Sensing of Urban Green Spaces: A Review.‚Äù Urban Forestry & Urban Greening 57: 126946.\n\n\n‚ÄúWhat Is SAR?‚Äù n.d. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar."
  },
  {
    "objectID": "week_6.html#application",
    "href": "week_6.html#application",
    "title": "Week6 - Google Earth Engine",
    "section": "Application",
    "text": "Application\n\nGoogle Earth Engine for geo-big data applications and ethics concern\nThe study of Haifa et. al (Tamiminia et al. 2020) uses Google Earth Engine to track and study changes in land use within the Brazilian Amazon rainforest. This project relies on satellite images from Landsat and Sentinel-2 to observe deforestation and the deterioration of the forest. To categorize different types of land and notice changes over periods, advanced machine learning techniques, including Random Forest and Support Vector Machines can be used. The goal is to provide precise, current data on how the land is beingndefined used, which is crucial for keeping an eye on the environment and helping conservation efforts.\nStephen R.J. Sheppard* and Petr Cizek (Sheppard and Cizek 2009) explores the ethical implications of using Google Earth‚Äôs visualization capabilities. It delves into the risks and benefits associated with the participatory use of virtual globes by experts and laypeople. This paper (Sheppard and Cizek 2009) emphasizes the need for ethical frameworks and principles to guide the use of environmental visualisation techniques in the context of public policy and decision-making.\nThis contrast (Tamiminia et al. 2020; Sheppard and Cizek 2009)suggests a complementary relationship where ethical guidelines could enhance the responsible use of technologies like GEE in scientific and policy-making contexts."
  },
  {
    "objectID": "week_6.html#refelction",
    "href": "week_6.html#refelction",
    "title": "Week6 - Google Earth Engine",
    "section": "Refelction",
    "text": "Refelction\nAfter working through the given exercises, it is easily understood that, yep, Google Earth Engine (GEE) is indeed powerful for exploring spatial patterns and really great for going deeper into your datasets. The entire process ranged from file uploads of the shapefiles for Delhi and London to the filtering of data inside these shapefiles. This was represented graphically in a map through every step that was clear in its display, both editable and showed the differences in their layers. This feature shows the ability of GEE in the handling of complicated spatial data in an interactive and easy manner.\nHowever, it is worth noting that there are downsides to the platform, especially on stability issues. Sometimes the service can be termed as unreliable, especially when uploading the shapefiles, which can prove quite frustrating. Except for this sometimes observed drawback, the utility and power of GEE for facilitation in studies concerning spatial analysis are enormous. It is a very rich set of tools that Google Earth Engine presents to every person who wants to make his first forays into spatial data analysis, with just a few issues about server stability."
  },
  {
    "objectID": "week_6.html#references",
    "href": "week_6.html#references",
    "title": "Week6 - Google Earth Engine",
    "section": "References",
    "text": "References\n\n\n\n\nSheppard, Stephen RJ, and Petr Cizek. 2009. ‚ÄúThe Ethics of Google Earth: Crossing Thresholds from Spatial Data to Landscape Visualisation.‚Äù Journal of Environmental Management 90 (6): 2102‚Äì17.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and Brian Brisco. 2020. ‚ÄúGoogle Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.‚Äù ISPRS Journal of Photogrammetry and Remote Sensing 164: 152‚Äì70."
  },
  {
    "objectID": "week_7.html#algorithms",
    "href": "week_7.html#algorithms",
    "title": "Week7 - Classification I",
    "section": "Algorithms",
    "text": "Algorithms\nAmongst many classification algorithms, decision trees have proven to be efficient algorithms for classification of large datasets. A decision tree is a classification algorithm that automatically derives a hierarchy of partition rules with respect to a target attribute of a large dataset (Li and Claramunt 2006). Forest is build from single tree. Classification aims to Classify data into discrete categories based on certain features.\n\n\n\nA basic decision tree model with a binary target variable Y (0 or 1) and two continuous variables, x1 and x2 (0 to 1) (Song and Ying 2015).\n\n\nThree types of nodes exist: The root node, or decision node, which divides records into exclusive subsets, another internal nodes, or chance nodes, indicating available choices and linking parent and child nodes, and the leaf nodes, or end nodes, signifying the outcome of decisions. To avoid overfit, decision trees require stopping rules Li and Claramunt (2006), such as minimum records in a leaf or node before splitting, and maximum leaf depth from the root.\nRandom Forests improve decision tree accuracy by generating multiple trees through random samples and feature subsets, thus enhancing diversity and reducing overfitting. This method involves creating a forest where each tree contributes to the final prediction through majority voting, while unselected data offers an unbiased error estimate. The ensemble approach allows trees to fully grow without pruning, and the number of features evaluated at each split is often the square root of the total features, further ensuring robust predictions.\nSupport Vector Machine (SVM) is a machine learning tool for categorizing data by drawing an optimal separating line (or higher-dimensional plane) to distinguish different data points. It aims for the widest margin between the nearest points of any category, known as support vectors. SVM‚Äôs effectiveness is fine-tuned using two parameters: C, which controls the margin‚Äôs strictness and focus on difficult points, and Gamma, which dictates the influence of each data point, with higher values emphasizing closer points. For data not linearly separable, SVM employs the kernel trick to manipulate the data into a separable form."
  },
  {
    "objectID": "week_7.html#coding-with-gee",
    "href": "week_7.html#coding-with-gee",
    "title": "Week7 - Classification I",
    "section": "Coding with GEE",
    "text": "Coding with GEE\nThis practice employs GEE for land classification by first filtering, clipping, and reducing the image, then adding point-based feature collections representing different land types to the satellite map.\n\n\n\nPoints represent: River (blue), Park (green), Soil (brown), Buildings (grey).\n\n\nThe second step involves using CART for training and classification. After separately classifying soil and buildings, subtract the two images to identify potential unused land areas.\n \nThe unused land in London appears minimal, indicating a well-developed area.\n\n\n\nBrown indicates the possible un-used land\n\n\nThe final image encapsulates the outcome of the analysis, presenting a comprehensive map that starkly contrasts the unused land in London. The brown patches represent potential undeveloped land amidst a sea of green, which indicates well-utilized space. Overall, the process through GEE is really helpful for handling remote sensing data in geo field."
  },
  {
    "objectID": "week_7.html#application",
    "href": "week_7.html#application",
    "title": "Week7 - Classification I",
    "section": "Application",
    "text": "Application\nThe method provided by Yan-yan SONG and Ying LU(Song and Ying 2015) segments a population into branches forming an inverted tree, comprising a root, internal, and leaf nodes. It‚Äôs a non-parametric algorithm, effectively handling large, complex datasets without requiring a complex parametric framework. When combine the population into the spatial analysis, here is an example. LandScan USA(Bhaduri et al. 2007) offers high-resolution population distribution data essential for socio-environmental research, public health, homeland security, and policy-making. This data supports operational activities, scientific analyses, and studies on population dynamics over time and space. It‚Äôs data is pivotal for identifying vulnerable groups like the elderly or low-income communities, guiding targeted policy development. Additionally, it serves emergency management and homeland security by offering detailed population distribution insights, enhancing disaster response and resource planning. Applying classification in these areas accelerates target group identification and reduces budget requirements."
  },
  {
    "objectID": "week_7.html#reflection",
    "href": "week_7.html#reflection",
    "title": "Week7 - Classification I",
    "section": "Reflection",
    "text": "Reflection\nIt is evident that Google Earth Engine (GEE) offers a robust platform for geographical data analysis and land classification. The integration of GEE for categorizing land into specific features such as rivers, parks, soil, and buildings, and its subsequent application for identifying potential unused land in urban areas, as demonstrated in London, showcases the dynamic capabilities of GEE.\nThe methodology employed leverages the Classification and Regression Trees (CART) for training and classification purposes, ensuring a detailed, systematic approach to land assessment. The practice of subtracting the classified images of soil and buildings to extract information about unused land areas reflects an innovative approach to urban planning and land management.\nHowever, the analysis reveals that unused land in London is scarce, underscoring the city‚Äôs extensive development. This observation could have significant implications for urban policy and development strategies, as it highlights the limited scope for expansion and the potential need for creative land use planning. It suggests that future urban development may need to focus on vertical expansion or redevelopment of existing areas rather than horizontal sprawl, which is often less sustainable.\nMoreover, the use of GEE for such analyses provides an educational insight into the potential of machine learning and big data in enhancing my understanding of urban landscapes. By using zonal statistics, regional and neighborhood reduction techniques, and regression models, GEE allows for a comprehensive evaluation of land utilization, which is critical for sustainable urban planning.\nThe practical provides a learning opportunity to delve deeper into the spatial and spectral intricacies of our world, enabling the development of more informed, data-driven decisions in urban and environmental planning."
  },
  {
    "objectID": "week_7.html#references",
    "href": "week_7.html#references",
    "title": "Week7 - Classification I",
    "section": "References",
    "text": "References\n\n\n\n\nBhaduri, Budhendra, Edward Bright, Phillip Coleman, and Marie L Urban. 2007. ‚ÄúLandScan USA: A High-Resolution Geospatial and Temporal Modeling Approach for Population Distribution and Dynamics.‚Äù GeoJournal 69: 103‚Äì17.\n\n\nJordan, Michael I, and Tom M Mitchell. 2015. ‚ÄúMachine Learning: Trends, Perspectives, and Prospects.‚Äù Science 349 (6245): 255‚Äì60.\n\n\nLi, Xiang, and Christophe Claramunt. 2006. ‚ÄúA Spatial Entropy-Based Decision Tree for Classification of Geographical Information.‚Äù Transactions in GIS 10 (3): 451‚Äì67.\n\n\nSong, Yan-Yan, and LU Ying. 2015. ‚ÄúDecision Tree Methods: Applications for Classification and Prediction.‚Äù Shanghai Archives of Psychiatry 27 (2): 130."
  },
  {
    "objectID": "week_8.html#summary",
    "href": "week_8.html#summary",
    "title": "Week8 - Classification The Big Questions (Lecture 6 continued) and Accuracy",
    "section": "Summary",
    "text": "Summary\n\nObject-Based Image Analysis (OBIA) simplifies imagery into significant objects by merging adjacent pixels with similar texture and color, focusing on object shapes to form ‚Äúsuperpixels‚Äù through either similarity or difference.\nSimple Linear Iterative Clustering (SLIC) Algorithm: This widely-used technique creates superpixels via Simple Linear Iterative Clustering. It initializes by distributing points across an image, then refines their placement through several iterations based on spatial distance and color difference.\nCompactness and Transformation: In SLIC, compactness determines superpixel shape‚Äîhigh values produce squarer shapes by prioritizing space, whereas low values favor color similarity, resulting in irregular shapes. It employs the LAB color space for processing, enhancing color perception accuracy by distinguishing luminance from color channels.\n\n\nSub pixel analysis\n\nEnd Members: These are pure spectral signatures in remote sensing that represent specific ground materials or objects, like water, vegetation, and soil.\nPixel Modeling: This aims to ascertain the composition of these end members within a single pixel‚Äôs image.\nCalculation Method: The equation‚Äôs left side depicts the pixel‚Äôs spectral signature across bands 3 and 4, under the constraint that end member fractions sum to one. The matrix in the center contains the end members‚Äô spectral signatures for bands 3 and 4, plus a row of ones for the sum-to-one rule. The right side aims to determine each end member‚Äôs fraction within the pixel. Fractions are computed by creating a new matrix where the left side is the fraction matrix, revealing the proportions of various end members.\n\n\n\n\nAfter using error matrix to filter all results and add together to visualize the sub pixel classification, it breaks down images to understand the mix of elements (like green area, urban blocks and water) within each pixel. I guess it will be useful for spotting changes at the greenery edge or urban deve\n\n\n\n\nAssess accuracy\nIn machine learning, accuracy assessment measures the congruence between a model‚Äôs predictions and actual outcomes, encompassing True Positive (TP) where the model accurately predicts the positive class, False Positive (FP) where it mistakenly predicts the positive class, True Negative (TN) where it accurately predicts the negative class, and False Negative (FN) where it incorrectly predicts the negative class.\nThe assessment includes producer‚Äôs accuracy \\[\\frac{TP}{TP+FN}\\] user‚Äôs accuracy \\[\\frac{TP}{TP+FP}\\] overall accuracy \\[\\frac{TP+TN}{TP+TN+FP+FN}\\] to measure model performance comprehensively.\n\n\nObject-based modeling\nObject-Based Image Analysis (OBIA) processes and analyzes imagery by segmenting it into meaningful, discrete objects or segments rather than analyzing pixel by pixel. This approach is grounded in the concept that contiguous pixels with similar characteristics (such as color, texture, or tone) can be grouped to form a larger, coherent object that represents a real-world feature, such as a field, forest patch, body of water, or urban area.\n\n\n\nThe OBIA (Object-Based Image Analysis) classification highlighted areas of water, urban development, and green spaces. However, the results appear somewhat abstract, with classifications emerging from clusters or groups of pixels.\n\n\n\n\nCross validation.\nIn geographic data analysis, data is typically divided into training and testing segments to evaluate model performance. Waldo Tobler‚Äôs (Geography 2024) ‚ÄúFirst Law of Geography‚Äù posits that closer objects are more similar than distant ones, suggesting a potential overlap issue where training data too close to the test data might inadvertently ‚Äòleak‚Äô information. To mitigate this, cross-validation, which partitions data into several ‚Äúfolds,‚Äù employs k-means clustering. This method clusters data points based on their proximity, ensuring that the training and testing sets are sufficiently separated to prevent overlap and maintain the integrity of the evaluation process."
  },
  {
    "objectID": "week_8.html#application",
    "href": "week_8.html#application",
    "title": "Week8 - Classification The Big Questions (Lecture 6 continued) and Accuracy",
    "section": "Application",
    "text": "Application\nAplin & Atkinson (Aplin and Atkinson 2001) present a methodology that significantly enhances the accuracy of land cover mapping through the integration of soft classification techniques at a sub-pixel level with traditional hard classification outcomes. This innovative approach effectively mitigates the challenge of mixed pixels‚Äîa prevalent issue in high-resolution imagery where single pixels may encapsulate multiple land cover types. By employing vector field boundaries for pixel segmentation and assigning accurate land cover classes to these segments, the method transitions soft classification results into precise, per-field hard classifications. This strategy not only surpasses traditional classification methods in accuracy but also offers a refined depiction of land cover by recognizing the partial membership of pixels to various classes.\nObject-based models, as opposed to traditional per-pixel or sub-pixel classification techniques, offer a novel approach for remote sensing imagery classification by utilizing geographical objects rather than individual pixels as the primary unit of analysis (Li et al. 2014). Conversely, Chen et al. (Chen et al. 2020) introduce a multi-objective optimization model designed to streamline satellite resource planning for extensive regional mapping. Their model, which aims to optimize the balance between maximizing coverage and minimizing resource expenditure, represents a strategic approach to managing the complexities associated with multi-satellite imaging tasks. Through incorporating considerations for imaging strip selection and angular adjustments, this methodology enhances both the decomposition of regions and the efficient allocation of satellite resources. Employing the non-dominated sorting genetic algorithm (NSGA-II) and the Vatti clipping algorithm, the study showcases significant methodological progress in orchestrating large-scale mapping efforts with multiple satellites.\nThey all highlight the dynamic applications and advancements in remote sensing technology, from detailed agricultural mapping to efficient large-scale regional mapping (hard to bring rea-life issue into application for me currently). The focus may be on state-of-the-art algorithms, but history is everlasting."
  },
  {
    "objectID": "week_8.html#reflection",
    "href": "week_8.html#reflection",
    "title": "Week8 - Classification The Big Questions (Lecture 6 continued) and Accuracy",
    "section": "Reflection",
    "text": "Reflection\nThis week‚Äôs lecture on advanced remote sensing techniques was a new field for me moving to how to extract more nuanced information from satellite images. The use of OBIA and superpixels through the SLIC algorithm has shifted my perspective on image classification. The pyramid of remote sensing has been built, hope to access to the complete project.\nSub-pixel analysis and understanding the specific spectral signatures of ground materials, or end members, have illustrated the incredible precision can be achieved. It‚Äôs like zooming in to the exact brushstrokes of different colors on a canvas to understand what‚Äôs really there. This can be a game-changer in areas like precision agriculture. The accuracy assessments and cross-validation methods reinforced the idea that models are tools that we must continually test and refine. They‚Äôre not infallible, but if used with diligence, they become powerful allies in understanding our environment.üå≥\nThese techniques could revolutionize environmental monitoring and resource management. While I might not use all these methods directly, the strategic thinking they require will undoubtedly shape how I approach complex problems and data analysis in the future."
  },
  {
    "objectID": "week_8.html#references",
    "href": "week_8.html#references",
    "title": "Week8 - Classification The Big Questions (Lecture 6 continued) and Accuracy",
    "section": "References",
    "text": "References\n\n\n\n\nAplin, P, and Peter M Atkinson. 2001. ‚ÄúSub-Pixel Land Cover Mapping for Per-Field Classification.‚Äù International Journal of Remote Sensing 22 (14): 2853‚Äì58.\n\n\nChen, Yaxin, Miaozhong Xu, Xin Shen, Guo Zhang, Zezhong Lu, and Junfei Xu. 2020. ‚ÄúA Multi-Objective Modeling Method of Multi-Satellite Imaging Task Planning for Large Regional Mapping.‚Äù Remote Sensing 12 (3): 344.\n\n\nGeography, GIS. 2024. ‚ÄúTobler‚Äôs First Law of Geography.‚Äù 2024. https://gisgeography.com/tobler-first-law-of-geography/.\n\n\nLi, Miao, Shuying Zang, Bing Zhang, Shanshan Li, and Changshan Wu. 2014. ‚ÄúA Review of Remote Sensing Image Classification Techniques: The Role of Spatio-Contextual Information.‚Äù European Journal of Remote Sensing 47 (1): 389‚Äì411."
  },
  {
    "objectID": "week_9.html#summary",
    "href": "week_9.html#summary",
    "title": "Week9 - Synthetic Aperture Radar (SAR)",
    "section": "Summary",
    "text": "Summary\nSAR, a radar device, measures the intensity and phase of backscattered signals across azimuth and range directions, correlating time bins with Earth‚Äôs locations. The ‚ÄòSynthetic Aperture‚Äô uses sensor motion to simulate a large antenna, ensuring adequate azimuth resolution (Google 2024). However, it is not perfect.\nSAR imaging has its drawbacks, including speckle noise, which gives a ‚Äòsalt-and-pepper‚Äô look to uniform areas due to the coherent microwave radiation, challenging to fully eliminate. Additionally, SAR backs-cattering varies with the incidence angle, making targets appear differently across the swath‚Äôs near and far ranges. Terrain relief also significantly impacts backscattering, with slope orientation causing foreshortening or shadowing, depending on its angle relative to the SAR (Google 2024).\n\nPolarization in Radar Systems(Natural Resources Canada 2024)\n\nRadar systems create polarized waves using antennas designed to transmit and receive electromagnetic (EM) waves of specific polarization. Different antenna types (horns, waveguides, dipoles, patches) ensure waves are polarized as intended. There are two types of polarization. Linear Polarization: Horizontal (H) and Vertical (V). Circular Polarization: Right Hand Circular (R) and Left Hand Circular (L), useful in weather radars. These are very abstract contents, it is important to try it in practical by myself.\nChange detection in SAR compares images from different times to spot changes, using statistical methods rather than direct subtraction to account for SAR‚Äôs unique characteristics. For instance, analyzing pre- and post-disaster SAR images helps assess damage from floods or conflicts. Statistical techniques, like t-tests, evaluate differences between two data-sets, revealing changes such as disaster impacts. Standard deviation measures data variability in SAR images over time, pinpointing areas with significant changes or unusual activity."
  },
  {
    "objectID": "week_9.html#week-practical",
    "href": "week_9.html#week-practical",
    "title": "Week9 - Synthetic Aperture Radar (SAR)",
    "section": "Week practical",
    "text": "Week practical\n\nApply Change Detection (east London estuary) to monitor differences in structures over time revealed that the coastline, potentially affected by a tsunami, saw minimal impact on buildings. Surprisingly, the significant changes were observed mostly in agricultural areas, rather than in built-up structures."
  },
  {
    "objectID": "week_9.html#application",
    "href": "week_9.html#application",
    "title": "Week9 - Synthetic Aperture Radar (SAR)",
    "section": "Application",
    "text": "Application\n\nCasa study: Google Earth Engine app using Sentinel 1 SAR and deep learning for ocean seep methane detection and monitoring (@ Hern√°ndez-Ham√≥n et al. 2023)\n\nThis study explores the application of deep learning, particularly deep belief networks (DBNs), as a novel approach to develop a global road safety performance function (SPF). Traditional regression models, while useful, face limitations in capturing the complex, non-linear relationships inherent in crash data across different highways and regions. The incorporation of DBNs promises a more flexible and accurate modeling framework, capable of addressing these challenges by leveraging the power of deep learning DiGiacomo et al. (2004).\nThe field of machine learning, especially artificial neural networks (ANNs) (@ Hern√°ndez-Ham√≥n et al. 2023), has made significant strides in simulating human cognitive functions and solving complex pattern recognition problems. However, traditional ANNs (DiGiacomo et al. 2004) struggle with non-convex optimization and require extensive training data, limiting their applicability. Deep learning (DL), through structures like DBNs, offers a solution by efficiently learning feature representations without needing labeled training data, showcasing superior performance in various domains, including road safety studies.\n\nThe investigation into using DBNs for developing global SPFs has demonstrated the model‚Äôs capability to surpass traditional regression-based approaches in accuracy and generalization. This research paves the way for future explorations into deep learning applications in road safety, promising more sophisticated and adaptable models for predicting and mitigating crash risks.\n\nMore SAR continues from week 4\n\nDrawback?\nInterferometry Synthetic Aperture Radar (InSAR)"
  },
  {
    "objectID": "week_9.html#reflection",
    "href": "week_9.html#reflection",
    "title": "Week9 - Synthetic Aperture Radar (SAR)",
    "section": "Reflection",
    "text": "Reflection\nI found the SAR technology mixed with deep learning like DBNs pretty amazing. It‚Äôs like having a high-tech camera in the sky that can see changes on Earth, and using some smart computer tricks to make sense of all those pictures. The idea of using this tech for spotting methane leaks with the help of Google Earth is like a real-life game of ‚ÄúWhere‚Äôs Waldo?‚Äù for environmental protection, and that‚Äôs super important for looking after our planet.\nThe bit about polarization in radars was a bit tricky, but think of it as different ways to send out radio waves to catch different types of information. Some waves are good for one thing, like weather forecasting, while others are better for something else. Using stats to spot changes in SAR images is like using a magnifying glass to find clues in a detective story. It‚Äôs all about paying attention to the details and figuring out what‚Äôs changed, which can help in seeing if a flood has happened or if a building has been built. And Deep learning, which is like teaching computers to learn and think a bit like us, is shaking things up in how we predict road safety. Instead of just guessing, we‚Äôre teaching computers to make smarter predictions, which is like upgrading from a flip phone to a smartphone.\nEven if I don‚Äôt use this exact SAR or deep learning stuff in my job someday, understanding how to collect data carefully and analyze it is going to be really useful, no matter what I end up doing. It‚Äôs like learning the rules of the game before you start playing."
  },
  {
    "objectID": "week_9.html#references",
    "href": "week_9.html#references",
    "title": "Week9 - Synthetic Aperture Radar (SAR)",
    "section": "References",
    "text": "References\n\n\n\n\nDiGiacomo, Paul M., Libe Washburn, Benjamin Holt, and Burton H. Jones. 2004. ‚ÄúCoastal Pollution Hazards in Southern California Observed by SAR Imagery: Stormwater Plumes, Wastewater Plumes, and Natural Hydrocarbon Seeps.‚Äù Marine Pollution Bulletin 49 (11): 1013‚Äì24. https://doi.org/https://doi.org/10.1016/j.marpolbul.2004.07.016.\n\n\nGoogle. 2024. ‚ÄúSAR Basics with the Earth Engine.‚Äù https://developers.google.com/earth-engine/tutorials/community/sar-basics.\n\n\nHern√°ndez-Ham√≥n, Hernando, Paula Zapata Ramƒ±ÃÅrez, Maycol Zaraza, and Aaron Micallef. 2023. ‚ÄúGoogle Earth Engine App Using Sentinel 1 SAR and Deep Learning for Ocean Seep Methane Detection and Monitoring.‚Äù Remote Sensing Applications: Society and Environment 32: 101036.\n\n\nNatural Resources Canada. 2024. ‚ÄúPolarization Radar Systems: Tutorial on Radar Polarimetry.‚Äù https://natural-resources.canada.ca/maps-tools-and-publications/satellite-imagery-and-air-photos/satellite-imagery-products/educational-resources/tutorial-radar-polarimetry/polarization-radar-systems/9567."
  }
]